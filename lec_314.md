## `15-851 Lecture: Information-Theoretic Randomized Streaming Lower Bounds, With a Focus on Norm Estimation`
#### **github.com/DolevArtzi**
#### **3.14.24**
### Link to something cool but unrelated to this lecture: [CountSketch `scipy.linalg` link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.clarkson_woodruff_transform.html)
- **2022 Scribe Notes**: [[part 1](https://www.cs.cmu.edu/afs/cs/user/dwoodruf/www/teaching/15859-fall22/scribeLec19.pdf)] [[part2](https://www.cs.cmu.edu/afs/cs/user/dwoodruf/www/teaching/15859-fall22/scribeLec20.pdf)]

### Part 1 (Add first 25 min)
> ## <ins> **Estimating $l_0$-norm**
> ### <ins> **Sparse $x$**
> Given a sparse $x$, say with $l_0$ norm in $O(\frac{1}{\epsilon^2})$, we can find the $l_0$ norm with the **algorithm to recover a sparse vector**, which works by maintaining the product of $x$ with a **Vandermonde matrix** in the stream
> We can also apply the *tail guarantee* from **CountSketch** to find $l_0$:
> #### <ins> **CountSketch Tail Guarantee** 
> `CS` approximates every coordinate $x_i$ of $x$ simultaneously up to an additive error of $O(\frac{||x_{-B/4}}{\sqrt{B}})$, with $x_{-B/4}$ being $x$ after zeroing out its top $B/4$ coordinates by magnitude. If $x$ is $B/4$-sparse, then $||x_{-B/4}||_2 = 0. Thus, the guarantee says that $$\text{whp., CountSketch gives the non-zero } x_i\text{'s exactly if } x \text{ is } B/4\text{-sparse} $$
> ### <ins> **Dense $x$**
> If $|x|_0 \gg O(1/\epsilon^2)$, we need sampling for space efficient estimation. Suppose we have a known estimator $Z$ which provides a range $Z \leq |x|_0 \leq 2Z$, then we can independently sample each coordinate of $x$ with probability $p = \frac{100}{Z\epsilon^2}$ and output the $l_0$ norm of the sampled coords, divided by $p$. 
> #### **Theorem 1.** *$\frac{|y|_0}{p}$ is a $(1\pm\epsilon)$-estimation of $|x|_0$ wp. at least $\frac{49}{50}$*
> *Proof*: 
> Let $Y_i$ be an irv for whether the $i$-th coordinate of $x$ is sampled. Let $y$ be the vector restricted to the coords for which $Y_i = 1$.
> $$E[|y|_0] = \sum_{i: x_i \neq 0} E[Y_i] = p|x|_0 \tag{defn, linearity}$$
> $$\geq \frac{100}{\epsilon^2}. \text{        }\therefore p|x|_0 = \frac{100}{z\epsilon^2} |x|_0 \geq \frac{100}{z\epsilon^2}z$$
> ##### $I(A;B)$: entropy of $A$ revealed by $B$
> #### <ins> **Entropy**
> $H(X\mid Y)$ is 1 when $X \perp Y$
> #### <ins> **Mutual Information**
> $$I(X,Y) = H(X) - H(X\mid Y)$$
> \- non-negative\
> \- proved that $H(X\mid Y) \leq H(X)$
> #### <ins> **Chain Rule for Mutual Information**
> $$I(X,Y;Z) = I(X;Z) + I(Y;Z\mid X)$$
> *Proof*:
> $$I(X,Y;Z) = H(X,Y) - H(X,Y\mid Z)$$
> $$= H(X) + H(Y\mid X) - H(X \mid Z) - H(Y\mid X,Z)$$
> $$= I(X;Z) + I(Y;Z\mid X)$$
> By induction, $$I(X_1,...,X_n;Z) = \sum_i I(X_i; Z \mid X_1, ..., X_{i-1})$$

> #### <ins> **Fano's Inequality**
> For any estimator $X': X\rightarrow Y \rightarrow X'$ with $P_e = P[X' \neq X]$, we have $$H(X \mid Y) \leq H(P_e) + P_e\log(|X| - 1)$$
> \- $X\rightarrow Y$ is sending $X$ across a *noisy channel*, and then we estimate it to obtain $X'$ from $Y$\
> \- $X \rightarrow Y \rightarrow X'$ is a **Markov chain**, meaning $X \perp X' \mid Y$\, or in other words, $(X \mid Y) \perp (X' \mid Y)$\
> \- Fano's inequality *lower bounds error by conditional entropy*
> when $X$ is a `bit`, $\log(|X| - 1) = 0$, so entropy of $P[\text{error}]$ is at least the entropy of $X$ given $Y$. 
> $$\textit{``Past and future are conditionally independent given the present"}$$
> See [proof](#proof-of-fanos-inequality-statement-of-inequalityins-fanos-inequality)

> #### <ins> **Data Processing Inequality**
> Suppose $X \rightarrow Y \rightarrow Z$ is a Markov chain. Then $$I(X;Y) \geq I(X;Z)$$
> *No clever combination of the data can improve estimation*\
> *Proof*:
> $$I(X; Y,Z) = I(X;Z) + I(X;Y\mid Z) \tag{chain rule for info.}$$
> $$= I(X;Y) + I(X;Z\mid Y) \tag{symmetry}$$
> Sts. $I(X;Z\mid Y) = 0$\
> $$I(X;Z\mid Y) = H(X\mid Y) - H(X\mid Y,Z)$$
> But given $Y$, $X \perp Z$, so $H(X\mid Y,Z) = H(X\mid Y)$.\
> Thus, **data processing inequality** $\implies H(X\mid Y) \leq H(X\mid Z)$

> #### *Proof of Fano's Inequality*: [statement of inequality](#ins-fanos-inequality)
> Let $E = \mathbf{1}_{X' \neq X}$.
> $$H(E,X\mid X') = H(X\mid X') + H(E\mid X,X') \tag{chain rule}$$
> $$= H(X\mid X') \tag{second term deterministic, entropy is 0}$$
> $$= H(E|X') + H(X|E,X') \leq H(P_e) + H(X|E,X')$$
> But $$H(X\mid E,X') \leq H(P_e) + H(X\mid E,X')  \tag{conditioning can't increase entropy}$$
> $$\leq (1- P_e) \cdot 0 + P_e \log_2(|X|-1) \tag{defn}$$
> Combining the above, $$H(X|X') \leq H(P_e) + P_e \log_2(|X| - 1)$$
> By [**data processing ineq.**](#ins-data-processing-inequality), $$H(X|Y)\leq H(X|X') \leq H(P_e) + P_e \log_2(|X| - 1)$$
> Sidenote, 
> $$H(X\mid E = 1) = \sum_x P[X = x \mid E=1]\log_2(\frac{1}{P[X = x\mid E = 1]}) \tag{defn}$$
> Here's what we did: we took the entropy of $E,X \mid X'$, and wrote it in two different ways:
>   1. $H(X\mid X') \leq H(P_e) + H(X\mid E,X')$
> 2. took the second term from above, and upper bounded it by $P_e\log_2(|X|-1)$
> 3. combined with the **dpl** to extend our upper bound to obtain [Fano's ineq.](#ins-fanos-inequality)

> #### <ins> **Tightness of Fano's Ineq.**
> \- suppose the dist. $p$ of $X$ satisfies $p_1 \geq p_2 \geq ... \geq p_n$\
> \- suppose $Y$ is constant, so $I(X;Y) = H(X) - H(X\mid Y) = 0$\
> \- then the best predictor of $X$ is $X=1$
> \- $P_e = P[X' \neq X] = 1-p_1$
> $$H(X\mid Y) \leq H(p_1) + (1-p_1)\log_2(n -1) \tag{prediction by Fano's, symmetry}$$
> But $H(X) = H(X\mid Y)$ and if $p_2 = p_3 = ... = p_n = \frac{1-p_1}{n-1}$, the inequality is tight
> For $X$ from $(p_1, \frac{1-p_1}{n-1}, ...)$,
> $$H(X) = \sum_i p_i \log(\frac{1p_i})\tag{defn}$$
> $$= p_1 \log(\frac{1}{p_1}) + \sum_{i > 1}\frac{1-p_1}{n-1}\log(\frac{n-1}{1-p_1}) \tag{log rules}$$
> $$= p_1 \log(\frac{1}{p_1}) = (1-p_1)\log(\frac{1}{1-p_1}) + (1-p_1)\log(n-1)$$
> $$= H(P-1) + (1-p_1)\log(n-1) \tag{Fano's}$$

### Part 2

### <ins> **Randomized One-Way Communication Complexity**
> **Index Problem**:
> * **Input**: $x \in \{0,1\}^n$, Alice sends a message $M$ to Bob, who has an index $j \in [n]$
> * **Output**: Given $M,j$ Bob should output $x_j$ w.p $\geq 2/3$
> **Prove**: For some inputs and coin tosses, $M$ must be $\Omega(n)$ bits long (so Alice can't do anything to improve in the worst case)
> *Proof*: Consider a uniform distribution $\mu$ on $X$. Alice sends $M$ to Bob. Bob's output is a guess, $X_j'$, to $X$
> $$\text{For all } j, P[X_j' = X_j] \geq \frac{2}{3}$$
> By Fano's ineq, for all $j$, $$H(X_j \mid M) \leq H(\frac{1}{3}) + \frac{1}{3}(\log_2 2 - 1) = H(\frac{1}{3})$$
> We have a Markov chain $X_j \rightarrow M \rightarrow X_j'$, as before\
>  **Main Takeaway**: *The entropy of $X_j$ given message, is at most the entropy of the error probability*.
> #### <ins> **One-Way Communication Complexity of Index**
> *Proof*:  **useful for [HW3P2]** Consider the mutual information $I(M;X)$\
> \- by the chain rule, $$I(X;M) = \sum_i I(X_i;M\mid X_{1,...,i-1})$$
> $$= \sum_i H(X_i \mid X_{<i}) - H(X_i \mid M, X_{\leq i}) \tag{$X$ indep. of prev. bits, entropy 1}$$
> Since the coordinates of $X$ are indep. bits, $H(X_i\mid X_{<i}) = 1$. Since conditioning can't increase entropy, $$H(X_i \mid M, X_{<i}) \leq H(X_i \mid M)$$
> So, $I(X;M) \geq n - \sum_i H(X_i \mid M) \geq n - H(\frac{1}{3})n$\
> Finally, translating it to an information-theoretic LB, we have $$|M| \geq H(M) \tag{u.a.r maximizes entropy, $\log(2^{|M|}) = |M|$}$$
> $$\geq I(X;M) = \Omega(n)$$

> #### <ins> **Typical Communication Reduction**
> **Alice**: input $a \in \{0,1\}^n$, creates stream $s(a)$\
> **Bob**: input $b$, creates stream $s(b)$\
> #### `Lower Bound Technique`:
> 1. Run streaming alg. on $s(a)$, transmit state of $Alg(s(a))$ to Bob
> 2. Bob computes $Alg(s(a),s(b))$
> 3. If Bob solves $g(a,b)$, space complexity of $Alg$ at least the [**one-way communication complexity**](#ins-one-way-communication-complexity-of-index) of $g$

> #### <ins> **Ex: Distinct Elements**
> **Given**: $a_1, ..., a_m$, each in $[n]$, how many *disinct* numbers are there?
> Recalling [index problem](#ins-one-way-communication-complexity-of-index)
> * Alice has a bit string $x$
> * Bob has an index $i \in [n]$
> * Bob wants to know if $x_i = 1$\
> #### `Reduction`:
> * $s(a) = i_1, ..., i_r$ where $i_j$ appears iff $x_{i_j} = 1$
> * $s(b) = i$
> * If $Alg(s(a),s(b)) = Alg(s(a)) + 1$, then $x_i = 0$. Otherwise, $x_i = 1$\
> * *The space complexity of $Alg$ is at least the one-way communication complexity of **index problem***

> #### <ins> **Augmented Index**
> * Alice has $x$
> * Bob has $i$ and $x_1,...,x_{i-1}$
> * Bob wants to learn $x_i$
> * Similar proof shows $\Omega(n)$ bound:\
> *Proof*: $$I(M;X) = \sum_i I(M; X_i \mid X_{<i})$$
> $$= \sum_i H(X_i \mid X_{<i}) - H(X_i \mid M, X_{<i})$$
> $$= n - \sum_i H(X_i \mid M, X_{<i}) \tag{Bob has $X_{<i}$, can use Fano's}$$
> By Fano's ineq, the summand is at most $H(\delta)$ if Bob can predict $X_i$ with probability at least $1 - \delta$.
> $$CC_\delta(\text{Aug-Index})\geq I(M; X) \geq n(1-H(\delta))$$

> #### <ins> **Ex: $\log n$ Bit Lower Bound for Estimating Norms**
> Previously [add_me](abc), we saw that the memory to estimate $p$-norm in a stream was $\frac{\log n}{\epsilon^2}$ for $0 < p < 2$. If $\epsilon = \theta(1)$, we have a logarithmic UB. We'll match this, with a $\Omega(\log n)$ LB.
> * **Alice**: $x \in \{0,1\}^{\log n}$ as an input to Augmented-Index
> * She creates a vector $v$ with a single coordinate equal to $\sum_j 10^j x_j$ (at most poly-$n$ values)
> * Alice sends Bob the state of the data stream algorithm after feeding in the input $v$
> * Bob wlog (can be `x[:i]` or `x[i+1:]`) knows the  coordinates of $x$ after $i$, and Bob wants $x_i$. 
> Bob is going to use the input sharing to his advantage. He creates a vector $w = \sum_{j>i} 10^j x_j$
> * Bob feeds $-w$ into the state of the algorithm. If $x_i = 1$, sum is at least $10^i$. If $x_i = 0$, sum is at most $1 + 10 + 100 + ... + 10^{i-1} \leq 10^i/9$. Bob exploits the gap to discover the binary value $x_i$\
> #### **Aside**: to represent an integer in $[n]$ takes $\log_2 n$ bits. However, if we just need to approximate it to within a factor of 2, only need to represent a number in powers of 2: $\{2,2^2,...,2^{\log_n}\} = \log(\log n + 1) = O(\log\log n)$ bits
> But, even still, we have separation between what Bob would see depending on $x_i$.\
> Therefore, you get an $\Omega(\log n)$ bounds for approximating $p$-norms\
> Finally, if the output of the streaming alg. is at least $10^i/2$, guess $x_i = 1$, o.w. guess $0$

> ### <ins> **$1/\epsilon^2$ Lower Bound for Estimating Norms**
> #### **Gap Hamming Problem**: We have two people, one has $x$, other has $y$, both length $n$ binary strings, guaranteed that Hamming distance, $\Delta$, follows:
> $$\Delta(x,y) > n/2 + 2\epsilon n \text{ or } \Delta(x,y) < n/2 + \epsilon n$$
> First attempt: randomly sample indices. With Chebyshev's, should be able to work out that $\Omega(\frac{1}{\epsilon^2})$ samples suffices
> Note that $|x - y|^2_2 = \Delta(x,y)$, Alice can send $Sx$ to Bob
> also another bound [Indyk, W], [W], [Jayram, Kumar, Sivakumar]
> Gives $\Omega(\epsilon^-2)$ LB for approximating **any** norm over binary strings
> If I can estimate a norm up to a $1+\epsilon$ factor, I can estimate the Hamming distance to within a $1+\epsilon$ factor. Since we have that special property about $\Delta$ in this case, we have separation. 

> #### <ins> **Overall LB for Norm Estimation**
> We know now that it's at least $\Omega(\log n + 1/\epsilon^2)$, it turns out that taking $\log n$ instances of Gap Hamming at different scales, and Bob will delete off some of them (outside of scope of this class), and the LBs can be strengthened for $\Omega(\log n + 1/\epsilon^2)$

> #### **Lower Bound for Gap Hamming** (`reduction from index`) **[JKS]**
> * **Alice**: Bit string $x$ of length $t := \theta(1/\epsilon^2)$
> * **Bob**: Index $i \in [t]$
> * Want to solve **index** by creating inputs to **gap Hamming**
> **Claim**: From answer to **Hamming**, can create answer to **index**\
> **Public Coin**: $r^1, ... r^t$, each in $\{0,1\}^t$, conditioning on the public coin in the original index proof would have also not affected the information, by independence. 
> * Bob creates input $b$ to gap Hamming from his index $i$. He sets $b_k = r^k_i$, so the $k$-th bit of $b$ is the $i$-th bit of $r^k$ 
> * Alice wants to create input $a \in \{0,1\}^t$ for gap Hamming. She looks at all $x_i $ s.t. $x_i = 1$. Let $i_1, i_2, ..., i_s$ be the positions where the corresponding bit in $x$ is 1. The $i$-th bit Alice defines is the majority of the $i$-th bits across the $r$'s (with the $i$-th bit 1?)\
> **Why does this work?** 
> * Suppose $x_i = 0$. Then $a \perp b$, as Alice isn't looking at 0 columns. If $x_i = 0$, $E[\Delta(a,b)] = \frac{1}{2}$
> * Suppose $x_i = 1$. Then Alice is taking a majority of bits which includes Bob's bit. This is the same of looking at the *tail of $\text{Bin}(t-1,1/2)$* since bits $i$ and $j \neq i $ are independent. You can use Sterling's approximation to get $\frac{1}{2} + \theta(\frac{1}{\sqrt{s}})$
> concentration (summing up indep. things) at $t/2$ in one case, $t/2 - \theta(\epsilon)$ in the other. So `gap Hamming` works for `index`.